{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "import torchwordemb\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from CNN_1d import CNN\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from utils import train, predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_vect = np.load('/home/hugoperrin/Bureau/Datasets/ToxicComment/Comment2Vec_train.npy')\n",
    "test_vect = np.load('/home/hugoperrin/Bureau/Datasets/ToxicComment/Comment2Vec_test.npy')\n",
    "\n",
    "Xtrain = pd.read_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/train.csv')\n",
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_labels = Xtrain[list_classes].values\n",
    "\n",
    "Xtest = pd.read_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/test.csv')\n",
    "final_id = Xtest['id']\n",
    "\n",
    "del Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 1, 100), (153164, 1, 100))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data for torch\n",
    "train_comments = train_vect.reshape(train_vect.shape[0],1,train_vect.shape[1])\n",
    "test_comments = test_vect.reshape(test_vect.shape[0],1,test_vect.shape[1])\n",
    "del train_vect, test_vect\n",
    "\n",
    "train_comments.shape, test_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Train test split\n",
    "# train_comments = train_vect[:130000,:,:]\n",
    "# valid_comments = train_vect[130001:145000,:,:]\n",
    "# test_comments = train_vect[145001:]\n",
    "\n",
    "# train_labels = target[:130000,:]\n",
    "# valid_labels = target[130001:145000,:]\n",
    "# test_labels = target[145001:,:]\n",
    "\n",
    "# del target, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2752\n",
      "Epoch: 1, step:   200, training loss: 0.1664\n",
      "Epoch: 1, step:   300, training loss: 0.1579\n",
      "Epoch: 2, step:   400, training loss: 0.1301\n",
      "Epoch: 2, step:   500, training loss: 0.1464\n",
      "Epoch: 2, step:   600, training loss: 0.1492\n",
      "Epoch: 3, step:   700, training loss: 0.1090\n",
      "Epoch: 3, step:   800, training loss: 0.1433\n",
      "Epoch: 3, step:   900, training loss: 0.1409\n",
      "Epoch: 4, step:  1000, training loss: 0.0857\n",
      "Epoch: 4, step:  1100, training loss: 0.1402\n",
      "Epoch: 4, step:  1200, training loss: 0.1412\n",
      "Epoch: 5, step:  1300, training loss: 0.0671\n",
      "Epoch: 5, step:  1400, training loss: 0.1381\n",
      "Epoch: 5, step:  1500, training loss: 0.1393\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1055\n",
      "Epoch: 1, step:   200, training loss: 0.0421\n",
      "Epoch: 1, step:   300, training loss: 0.0307\n",
      "Epoch: 2, step:   400, training loss: 0.0237\n",
      "Epoch: 2, step:   500, training loss: 0.0285\n",
      "Epoch: 2, step:   600, training loss: 0.0273\n",
      "Epoch: 3, step:   700, training loss: 0.0198\n",
      "Epoch: 3, step:   800, training loss: 0.0255\n",
      "Epoch: 3, step:   900, training loss: 0.0262\n",
      "Epoch: 4, step:  1000, training loss: 0.0158\n",
      "Epoch: 4, step:  1100, training loss: 0.0270\n",
      "Epoch: 4, step:  1200, training loss: 0.0238\n",
      "Epoch: 5, step:  1300, training loss: 0.0129\n",
      "Epoch: 5, step:  1400, training loss: 0.0249\n",
      "Epoch: 5, step:  1500, training loss: 0.0243\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2253\n",
      "Epoch: 1, step:   200, training loss: 0.1247\n",
      "Epoch: 1, step:   300, training loss: 0.1014\n",
      "Epoch: 2, step:   400, training loss: 0.0832\n",
      "Epoch: 2, step:   500, training loss: 0.0922\n",
      "Epoch: 2, step:   600, training loss: 0.0883\n",
      "Epoch: 3, step:   700, training loss: 0.0644\n",
      "Epoch: 3, step:   800, training loss: 0.0870\n",
      "Epoch: 3, step:   900, training loss: 0.0859\n",
      "Epoch: 4, step:  1000, training loss: 0.0528\n",
      "Epoch: 4, step:  1100, training loss: 0.0826\n",
      "Epoch: 4, step:  1200, training loss: 0.0843\n",
      "Epoch: 5, step:  1300, training loss: 0.0423\n",
      "Epoch: 5, step:  1400, training loss: 0.0838\n",
      "Epoch: 5, step:  1500, training loss: 0.0798\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.0777\n",
      "Epoch: 1, step:   200, training loss: 0.0189\n",
      "Epoch: 1, step:   300, training loss: 0.0159\n",
      "Epoch: 2, step:   400, training loss: 0.0136\n",
      "Epoch: 2, step:   500, training loss: 0.0129\n",
      "Epoch: 2, step:   600, training loss: 0.0128\n",
      "Epoch: 3, step:   700, training loss: 0.0090\n",
      "Epoch: 3, step:   800, training loss: 0.0122\n",
      "Epoch: 3, step:   900, training loss: 0.0098\n",
      "Epoch: 4, step:  1000, training loss: 0.0073\n",
      "Epoch: 4, step:  1100, training loss: 0.0110\n",
      "Epoch: 4, step:  1200, training loss: 0.0101\n",
      "Epoch: 5, step:  1300, training loss: 0.0049\n",
      "Epoch: 5, step:  1400, training loss: 0.0105\n",
      "Epoch: 5, step:  1500, training loss: 0.0098\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2152\n",
      "Epoch: 1, step:   200, training loss: 0.1161\n",
      "Epoch: 1, step:   300, training loss: 0.0995\n",
      "Epoch: 2, step:   400, training loss: 0.0807\n",
      "Epoch: 2, step:   500, training loss: 0.0906\n",
      "Epoch: 2, step:   600, training loss: 0.0873\n",
      "Epoch: 3, step:   700, training loss: 0.0658\n",
      "Epoch: 3, step:   800, training loss: 0.0880\n",
      "Epoch: 3, step:   900, training loss: 0.0850\n",
      "Epoch: 4, step:  1000, training loss: 0.0525\n",
      "Epoch: 4, step:  1100, training loss: 0.0851\n",
      "Epoch: 4, step:  1200, training loss: 0.0842\n",
      "Epoch: 5, step:  1300, training loss: 0.0424\n",
      "Epoch: 5, step:  1400, training loss: 0.0831\n",
      "Epoch: 5, step:  1500, training loss: 0.0849\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1096\n",
      "Epoch: 1, step:   200, training loss: 0.0467\n",
      "Epoch: 1, step:   300, training loss: 0.0414\n",
      "Epoch: 2, step:   400, training loss: 0.0318\n",
      "Epoch: 2, step:   500, training loss: 0.0336\n",
      "Epoch: 2, step:   600, training loss: 0.0329\n",
      "Epoch: 3, step:   700, training loss: 0.0218\n",
      "Epoch: 3, step:   800, training loss: 0.0317\n",
      "Epoch: 3, step:   900, training loss: 0.0305\n",
      "Epoch: 4, step:  1000, training loss: 0.0189\n",
      "Epoch: 4, step:  1100, training loss: 0.0286\n",
      "Epoch: 4, step:  1200, training loss: 0.0311\n",
      "Epoch: 5, step:  1300, training loss: 0.0151\n",
      "Epoch: 5, step:  1400, training loss: 0.0282\n",
      "Epoch: 5, step:  1500, training loss: 0.0298\n",
      "\n",
      "\n",
      "CPU times: user 1min 31s, sys: 27.1 s, total: 1min 58s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = pd.DataFrame(index=final_id)\n",
    "\n",
    "for target in list_classes:\n",
    "\n",
    "    labels_train = train_labels[:,list_classes.index(target)]\n",
    "    labels_train = labels_train.reshape(labels_train.shape[0],1)\n",
    "\n",
    "#     labels_valid = valid_labels[:,list_classes.index(target)]\n",
    "#     labels_valid = labels_valid.reshape(labels_valid.shape[0],1)\n",
    "\n",
    "#     labels_test = test_labels[:,list_classes.index(target)]\n",
    "#     labels_test = labels_test.reshape(test_labels.shape[0],1)\n",
    "\n",
    "    use_GPU = False\n",
    "\n",
    "    batch_size = 512\n",
    "    num_epoch = 5\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(train_comments), \n",
    "                                                   torch.FloatTensor(labels_train))\n",
    "\n",
    "#     valid_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(valid_comments), \n",
    "#                                                    torch.FloatTensor(labels_valid))\n",
    "\n",
    "    test_dataset = torch.FloatTensor(test_comments)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "#     valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "#                                                batch_size=batch_size,\n",
    "#                                                shuffle=False, \n",
    "#                                                num_workers = 8)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    net = CNN()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=0.00002, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0.9)\n",
    "\n",
    "    train(num_epoch, net, train_loader, optimizer, criterion, valid_loader=None)\n",
    "\n",
    "    predictions[target] = predict(net, test_loader)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.to_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/1st_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
