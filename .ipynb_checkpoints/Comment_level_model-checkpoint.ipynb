{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "import torchwordemb\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from CNN_1d import CNN\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from utils import train, predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = np.load('/home/hugoperrin/Bureau/Datasets/ToxicComment/Comment2Vec.npy')\n",
    "\n",
    "Xtrain = pd.read_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/train.csv')\n",
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "target = Xtrain[list_classes].values\n",
    "\n",
    "del Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 1, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data for torch\n",
    "data = data.reshape(data.shape[0],1,data.shape[1])\n",
    "data.shape\n",
    "# test_comments = test_comments.reshape(test_comments.shape[0],1,test_comments.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_comments = data[:130000,:,:]\n",
    "valid_comments = data[130001:145000,:,:]\n",
    "test_comments = data[145001:]\n",
    "\n",
    "train_labels = target[:130000,:]\n",
    "valid_labels = target[130001:145000,:]\n",
    "test_labels = target[145001:,:]\n",
    "\n",
    "del target, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.3259, validation loss: 0.24242\n",
      "Epoch: 1, step:   200, training loss: 0.2037, validation loss: 0.17444\n",
      "Epoch: 2, step:   300, training loss: 0.0770, validation loss: 0.16049\n",
      "Epoch: 2, step:   400, training loss: 0.1563, validation loss: 0.15300\n",
      "Epoch: 2, step:   500, training loss: 0.1549, validation loss: 0.14829\n",
      "Epoch: 3, step:   600, training loss: 0.1368, validation loss: 0.14550\n",
      "Epoch: 3, step:   700, training loss: 0.1483, validation loss: 0.14308\n",
      "Epoch: 4, step:   800, training loss: 0.0537, validation loss: 0.14172\n",
      "Epoch: 4, step:   900, training loss: 0.1472, validation loss: 0.14225\n",
      "Epoch: 4, step:  1000, training loss: 0.1416, validation loss: 0.13838\n",
      "Epoch: 5, step:  1100, training loss: 0.1184, validation loss: 0.13770\n",
      "Epoch: 5, step:  1200, training loss: 0.1416, validation loss: 0.13679\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1297, validation loss: 0.05245\n",
      "Epoch: 1, step:   200, training loss: 0.0482, validation loss: 0.04475\n",
      "Epoch: 2, step:   300, training loss: 0.0166, validation loss: 0.03452\n",
      "Epoch: 2, step:   400, training loss: 0.0330, validation loss: 0.02955\n",
      "Epoch: 2, step:   500, training loss: 0.0317, validation loss: 0.02831\n",
      "Epoch: 3, step:   600, training loss: 0.0265, validation loss: 0.02699\n",
      "Epoch: 3, step:   700, training loss: 0.0298, validation loss: 0.02633\n",
      "Epoch: 4, step:   800, training loss: 0.0101, validation loss: 0.02598\n",
      "Epoch: 4, step:   900, training loss: 0.0277, validation loss: 0.02557\n",
      "Epoch: 4, step:  1000, training loss: 0.0265, validation loss: 0.02501\n",
      "Epoch: 5, step:  1100, training loss: 0.0207, validation loss: 0.02493\n",
      "Epoch: 5, step:  1200, training loss: 0.0269, validation loss: 0.02450\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2604, validation loss: 0.17583\n",
      "Epoch: 1, step:   200, training loss: 0.1581, validation loss: 0.13144\n",
      "Epoch: 2, step:   300, training loss: 0.0537, validation loss: 0.11075\n",
      "Epoch: 2, step:   400, training loss: 0.1095, validation loss: 0.10174\n",
      "Epoch: 2, step:   500, training loss: 0.1048, validation loss: 0.09557\n",
      "Epoch: 3, step:   600, training loss: 0.0917, validation loss: 0.09145\n",
      "Epoch: 3, step:   700, training loss: 0.0959, validation loss: 0.08841\n",
      "Epoch: 4, step:   800, training loss: 0.0350, validation loss: 0.08630\n",
      "Epoch: 4, step:   900, training loss: 0.0930, validation loss: 0.08502\n",
      "Epoch: 4, step:  1000, training loss: 0.0885, validation loss: 0.08337\n",
      "Epoch: 5, step:  1100, training loss: 0.0723, validation loss: 0.08179\n",
      "Epoch: 5, step:  1200, training loss: 0.0863, validation loss: 0.08093\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1191, validation loss: 0.01914\n",
      "Epoch: 1, step:   200, training loss: 0.0199, validation loss: 0.01805\n",
      "Epoch: 2, step:   300, training loss: 0.0078, validation loss: 0.01668\n",
      "Epoch: 2, step:   400, training loss: 0.0185, validation loss: 0.01523\n",
      "Epoch: 2, step:   500, training loss: 0.0157, validation loss: 0.01394\n",
      "Epoch: 3, step:   600, training loss: 0.0141, validation loss: 0.01285\n",
      "Epoch: 3, step:   700, training loss: 0.0135, validation loss: 0.01198\n",
      "Epoch: 4, step:   800, training loss: 0.0046, validation loss: 0.01129\n",
      "Epoch: 4, step:   900, training loss: 0.0123, validation loss: 0.01078\n",
      "Epoch: 4, step:  1000, training loss: 0.0123, validation loss: 0.01025\n",
      "Epoch: 5, step:  1100, training loss: 0.0098, validation loss: 0.01002\n",
      "Epoch: 5, step:  1200, training loss: 0.0110, validation loss: 0.01004\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2367, validation loss: 0.16773\n",
      "Epoch: 1, step:   200, training loss: 0.1485, validation loss: 0.12507\n",
      "Epoch: 2, step:   300, training loss: 0.0480, validation loss: 0.10390\n",
      "Epoch: 2, step:   400, training loss: 0.1052, validation loss: 0.09558\n",
      "Epoch: 2, step:   500, training loss: 0.1005, validation loss: 0.09060\n",
      "Epoch: 3, step:   600, training loss: 0.0851, validation loss: 0.08711\n",
      "Epoch: 3, step:   700, training loss: 0.0917, validation loss: 0.08520\n",
      "Epoch: 4, step:   800, training loss: 0.0313, validation loss: 0.08358\n",
      "Epoch: 4, step:   900, training loss: 0.0903, validation loss: 0.08276\n",
      "Epoch: 4, step:  1000, training loss: 0.0889, validation loss: 0.08223\n",
      "Epoch: 5, step:  1100, training loss: 0.0744, validation loss: 0.08212\n",
      "Epoch: 5, step:  1200, training loss: 0.0839, validation loss: 0.08105\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1336, validation loss: 0.05298\n",
      "Epoch: 1, step:   200, training loss: 0.0448, validation loss: 0.04942\n",
      "Epoch: 2, step:   300, training loss: 0.0186, validation loss: 0.04564\n",
      "Epoch: 2, step:   400, training loss: 0.0372, validation loss: 0.04184\n",
      "Epoch: 2, step:   500, training loss: 0.0360, validation loss: 0.03910\n",
      "Epoch: 3, step:   600, training loss: 0.0305, validation loss: 0.03753\n",
      "Epoch: 3, step:   700, training loss: 0.0340, validation loss: 0.03633\n",
      "Epoch: 4, step:   800, training loss: 0.0124, validation loss: 0.03485\n",
      "Epoch: 4, step:   900, training loss: 0.0325, validation loss: 0.03456\n",
      "Epoch: 4, step:  1000, training loss: 0.0285, validation loss: 0.03345\n",
      "Epoch: 5, step:  1100, training loss: 0.0243, validation loss: 0.03433\n",
      "Epoch: 5, step:  1200, training loss: 0.0288, validation loss: 0.03276\n",
      "CPU times: user 1min 15s, sys: 23.7 s, total: 1min 38s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = pd.DataFrame(index=range(len(test_comments)))\n",
    "\n",
    "for target in list_classes:\n",
    "\n",
    "    labels_train = train_labels[:,list_classes.index(target)]\n",
    "    labels_train = labels_train.reshape(labels_train.shape[0],1)\n",
    "\n",
    "    labels_valid = valid_labels[:,list_classes.index(target)]\n",
    "    labels_valid = labels_valid.reshape(labels_valid.shape[0],1)\n",
    "\n",
    "    labels_test = test_labels[:,list_classes.index(target)]\n",
    "    labels_test = labels_test.reshape(test_labels.shape[0],1)\n",
    "\n",
    "    use_GPU = True\n",
    "\n",
    "    batch_size = 512\n",
    "    num_epoch = 5\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(train_comments), \n",
    "                                                   torch.FloatTensor(labels_train))\n",
    "\n",
    "    valid_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(valid_comments), \n",
    "                                                   torch.FloatTensor(labels_valid))\n",
    "\n",
    "    test_dataset = torch.FloatTensor(test_comments)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    net = CNN()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=0.00001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0.9)\n",
    "\n",
    "    train(num_epoch, net, train_loader, optimizer, criterion, valid_loader=valid_loader)\n",
    "\n",
    "    predictions[target] = predict(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
