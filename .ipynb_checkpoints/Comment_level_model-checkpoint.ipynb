{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "import torchwordemb\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from CNN_1d import CNN\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from utils import train, predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_vect = np.load('/home/hugoperrin/Bureau/Datasets/ToxicComment/Comment2Vec_train.npy')\n",
    "# test_vect = np.load('/home/hugoperrin/Bureau/Datasets/ToxicComment/Comment2Vec_test.npy')\n",
    "\n",
    "Xtrain = pd.read_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/train.csv')\n",
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "targets = Xtrain[list_classes].values\n",
    "\n",
    "# Xtest = pd.read_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/test.csv')\n",
    "# final_id = Xtest['id']\n",
    "\n",
    "del Xtrain #, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess data for torch\n",
    "train_vect = train_vect.reshape(train_vect.shape[0],1,train_vect.shape[1])\n",
    "# test_comments = test_vect.reshape(test_vect.shape[0],1,test_vect.shape[1])\n",
    "# del train_vect, test_vect\n",
    "\n",
    "# train_comments.shape, test_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train test split\n",
    "train_comments = train_vect[:130000,:,:]\n",
    "valid_comments = train_vect[130001:145000,:,:]\n",
    "test_comments = train_vect[145001:]\n",
    "\n",
    "train_labels = targets[:130000,:]\n",
    "valid_labels = targets[130001:145000,:]\n",
    "test_labels = targets[145001:,:]\n",
    "\n",
    "del targets, train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2810\n",
      "Epoch: 1, step:   200, training loss: 0.1730\n",
      "Epoch: 2, step:   300, training loss: 0.0698\n",
      "Epoch: 2, step:   400, training loss: 0.1509\n",
      "Epoch: 2, step:   500, training loss: 0.1504\n",
      "Epoch: 3, step:   600, training loss: 0.1333\n",
      "Epoch: 3, step:   700, training loss: 0.1444\n",
      "Epoch: 4, step:   800, training loss: 0.0518\n",
      "Epoch: 4, step:   900, training loss: 0.1418\n",
      "Epoch: 4, step:  1000, training loss: 0.1412\n",
      "Epoch: 5, step:  1100, training loss: 0.1192\n",
      "Epoch: 5, step:  1200, training loss: 0.1394\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1166\n",
      "Epoch: 1, step:   200, training loss: 0.0373\n",
      "Epoch: 2, step:   300, training loss: 0.0129\n",
      "Epoch: 2, step:   400, training loss: 0.0290\n",
      "Epoch: 2, step:   500, training loss: 0.0292\n",
      "Epoch: 3, step:   600, training loss: 0.0248\n",
      "Epoch: 3, step:   700, training loss: 0.0265\n",
      "Epoch: 4, step:   800, training loss: 0.0099\n",
      "Epoch: 4, step:   900, training loss: 0.0259\n",
      "Epoch: 4, step:  1000, training loss: 0.0251\n",
      "Epoch: 5, step:  1100, training loss: 0.0200\n",
      "Epoch: 5, step:  1200, training loss: 0.0253\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2283\n",
      "Epoch: 1, step:   200, training loss: 0.1178\n",
      "Epoch: 2, step:   300, training loss: 0.0446\n",
      "Epoch: 2, step:   400, training loss: 0.0957\n",
      "Epoch: 2, step:   500, training loss: 0.0955\n",
      "Epoch: 3, step:   600, training loss: 0.0820\n",
      "Epoch: 3, step:   700, training loss: 0.0890\n",
      "Epoch: 4, step:   800, training loss: 0.0306\n",
      "Epoch: 4, step:   900, training loss: 0.0852\n",
      "Epoch: 4, step:  1000, training loss: 0.0866\n",
      "Epoch: 5, step:  1100, training loss: 0.0694\n",
      "Epoch: 5, step:  1200, training loss: 0.0834\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.0721\n",
      "Epoch: 1, step:   200, training loss: 0.0197\n",
      "Epoch: 2, step:   300, training loss: 0.0070\n",
      "Epoch: 2, step:   400, training loss: 0.0160\n",
      "Epoch: 2, step:   500, training loss: 0.0147\n",
      "Epoch: 3, step:   600, training loss: 0.0136\n",
      "Epoch: 3, step:   700, training loss: 0.0119\n",
      "Epoch: 4, step:   800, training loss: 0.0038\n",
      "Epoch: 4, step:   900, training loss: 0.0107\n",
      "Epoch: 4, step:  1000, training loss: 0.0121\n",
      "Epoch: 5, step:  1100, training loss: 0.0086\n",
      "Epoch: 5, step:  1200, training loss: 0.0106\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2205\n",
      "Epoch: 1, step:   200, training loss: 0.1254\n",
      "Epoch: 2, step:   300, training loss: 0.0470\n",
      "Epoch: 2, step:   400, training loss: 0.0973\n",
      "Epoch: 2, step:   500, training loss: 0.0941\n",
      "Epoch: 3, step:   600, training loss: 0.0826\n",
      "Epoch: 3, step:   700, training loss: 0.0904\n",
      "Epoch: 4, step:   800, training loss: 0.0316\n",
      "Epoch: 4, step:   900, training loss: 0.0883\n",
      "Epoch: 4, step:  1000, training loss: 0.0867\n",
      "Epoch: 5, step:  1100, training loss: 0.0726\n",
      "Epoch: 5, step:  1200, training loss: 0.0862\n",
      "\n",
      ">> LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1022\n",
      "Epoch: 1, step:   200, training loss: 0.0458\n",
      "Epoch: 2, step:   300, training loss: 0.0179\n",
      "Epoch: 2, step:   400, training loss: 0.0337\n",
      "Epoch: 2, step:   500, training loss: 0.0330\n",
      "Epoch: 3, step:   600, training loss: 0.0288\n",
      "Epoch: 3, step:   700, training loss: 0.0302\n",
      "Epoch: 4, step:   800, training loss: 0.0106\n",
      "Epoch: 4, step:   900, training loss: 0.0305\n",
      "Epoch: 4, step:  1000, training loss: 0.0288\n",
      "Epoch: 5, step:  1100, training loss: 0.0247\n",
      "Epoch: 5, step:  1200, training loss: 0.0276\n",
      "\n",
      "\n",
      "CPU times: user 1min 14s, sys: 24.1 s, total: 1min 38s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = pd.DataFrame(index=range(len(test_comments)))\n",
    "\n",
    "for target in list_classes:\n",
    "    \n",
    "    print(\"Estimation of {}\".format(target))\n",
    "\n",
    "    labels_train = train_labels[:,list_classes.index(target)]\n",
    "    labels_train = labels_train.reshape(labels_train.shape[0],1)\n",
    "\n",
    "    labels_valid = valid_labels[:,list_classes.index(target)]\n",
    "    labels_valid = labels_valid.reshape(labels_valid.shape[0],1)\n",
    "\n",
    "    labels_test = test_labels[:,list_classes.index(target)]\n",
    "    labels_test = labels_test.reshape(test_labels.shape[0],1)\n",
    "\n",
    "    use_GPU = True\n",
    "\n",
    "    batch_size = 512\n",
    "    num_epoch = 5\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(train_comments), \n",
    "                                                   torch.FloatTensor(labels_train))\n",
    "\n",
    "    valid_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(valid_comments), \n",
    "                                                   torch.FloatTensor(labels_valid))\n",
    "\n",
    "    test_dataset = torch.FloatTensor(test_comments)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    net = CNN()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=0.00002, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0.9)\n",
    "\n",
    "    train(num_epoch, net, train_loader, optimizer, criterion, valid_loader=None, use_GPU=use_GPU)\n",
    "\n",
    "    predictions[target] = predict(net, test_loader, use_GPU=use_GPU)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.063295737412642966"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0\n",
    "\n",
    "for i in range(len(list_classes)):\n",
    "    score += log_loss(test_labels[:,i],predictions.iloc[:,i])*(1/len(list_classes))\n",
    "    \n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions.to_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/1st_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
