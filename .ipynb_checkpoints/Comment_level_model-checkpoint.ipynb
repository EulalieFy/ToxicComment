{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "import torchwordemb\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from CNN_1d import CNN\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from utils import train, predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = np.load('/home/hugoperrin/Bureau/Datasets/ToxicComment/Comment2Vec.npy')\n",
    "\n",
    "Xtrain = pd.read_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/train.csv')\n",
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "target = Xtrain[list_classes].values\n",
    "\n",
    "del Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 1, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data for torch\n",
    "data = data.reshape(data.shape[0],1,data.shape[1])\n",
    "data.shape\n",
    "# test_comments = test_comments.reshape(test_comments.shape[0],1,test_comments.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_comments = data[:130000,:,:]\n",
    "valid_comments = data[130001:145000,:,:]\n",
    "test_comments = data[145001:]\n",
    "\n",
    "train_labels = target[:130000,:]\n",
    "valid_labels = target[130001:145000,:]\n",
    "test_labels = target[145001:,:]\n",
    "\n",
    "del target, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> PROCESSING LEARNING: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2418, validation loss: 0.17012\n",
      "Epoch: 1, step:   200, training loss: 0.1520, validation loss: 0.12418\n",
      "Epoch: 2, step:   300, training loss: 0.0519, validation loss: 0.10701\n",
      "Epoch: 2, step:   400, training loss: 0.1067, validation loss: 0.10026\n",
      "Epoch: 2, step:   500, training loss: 0.1055, validation loss: 0.09603\n",
      "Epoch: 3, step:   600, training loss: 0.0908, validation loss: 0.09380\n",
      "Epoch: 3, step:   700, training loss: 0.0985, validation loss: 0.09083\n",
      "Epoch: 4, step:   800, training loss: 0.0335, validation loss: 0.08907\n",
      "Epoch: 4, step:   900, training loss: 0.0945, validation loss: 0.08704\n",
      "Epoch: 4, step:  1000, training loss: 0.0921, validation loss: 0.08535\n",
      "Epoch: 5, step:  1100, training loss: 0.0740, validation loss: 0.08413\n",
      "Epoch: 5, step:  1200, training loss: 0.0893, validation loss: 0.08285\n",
      "Epoch: 6, step:  1300, training loss: 0.0247, validation loss: 0.08255\n",
      "Epoch: 6, step:  1400, training loss: 0.0870, validation loss: 0.08148\n",
      "Epoch: 6, step:  1500, training loss: 0.0880, validation loss: 0.08096\n",
      "\n",
      "\n",
      "CPU times: user 16.3 s, sys: 4.84 s, total: 21.1 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "target = 'obscene'\n",
    "\n",
    "labels_train = train_labels[:,list_classes.index(target)]\n",
    "labels_train = labels_train.reshape(labels_train.shape[0],1)\n",
    "\n",
    "labels_valid = valid_labels[:,list_classes.index(target)]\n",
    "labels_valid = labels_valid.reshape(labels_valid.shape[0],1)\n",
    "\n",
    "labels_test = test_labels[:,list_classes.index(target)]\n",
    "labels_test = labels_test.reshape(test_labels.shape[0],1)\n",
    "\n",
    "use_GPU = True\n",
    "\n",
    "batch_size = 512\n",
    "num_epoch = 6\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(train_comments), \n",
    "                                               torch.FloatTensor(labels_train))\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(valid_comments), \n",
    "                                               torch.FloatTensor(labels_valid))\n",
    "\n",
    "test_dataset = torch.FloatTensor(test_comments)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers = 8)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False, \n",
    "                                           num_workers = 8)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False, \n",
    "                                           num_workers = 8)\n",
    "\n",
    "net = CNN()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.00001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0.9)\n",
    "\n",
    "train(num_epoch, net, train_loader, optimizer, criterion, valid_loader=valid_loader)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.089223646124598655"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(labels_test, predict(net, test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
