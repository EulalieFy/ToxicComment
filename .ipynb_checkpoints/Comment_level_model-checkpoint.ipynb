{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "import torchwordemb\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from CNN_1d import CNN\n",
    "\n",
    "sys.path.append('/home/hugoperrin/Bureau/Data science/Kaggle/ToxicComment/Models/')\n",
    "from utils import train, predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_vect = np.load('/home/hugoperrin/Bureau/Datasets/ToxicComment/Comment2Vec_train.npy')\n",
    "# test_vect = np.load('/home/hugoperrin/Bureau/Datasets/ToxicComment/Comment2Vec_test.npy')\n",
    "\n",
    "Xtrain = pd.read_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/train.csv')\n",
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "targets = Xtrain[list_classes].values\n",
    "\n",
    "# Xtest = pd.read_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/test.csv')\n",
    "# final_id = Xtest['id']\n",
    "\n",
    "del Xtrain #, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess data for torch\n",
    "train_vect = train_vect.reshape(train_vect.shape[0],1,train_vect.shape[1])\n",
    "# test_comments = test_vect.reshape(test_vect.shape[0],1,test_vect.shape[1])\n",
    "# del train_vect, test_vect\n",
    "\n",
    "# train_comments.shape, test_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Train test split\n",
    "train_comments = train_vect[:130000,:,:]\n",
    "valid_comments = train_vect[130001:145000,:,:]\n",
    "test_comments = train_vect[145001:]\n",
    "\n",
    "train_labels = targets[:130000,:]\n",
    "valid_labels = targets[130001:145000,:]\n",
    "test_labels = targets[145001:,:]\n",
    "\n",
    "del targets, train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimation of toxic:\n",
      "\n",
      ">> Learning: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.3198, validation loss: 0.24034\n",
      "Epoch: 2, step:   200, training loss: 0.1420, validation loss: 0.17362\n",
      "Epoch: 3, step:   300, training loss: 0.0783, validation loss: 0.15958\n",
      "Epoch: 4, step:   400, training loss: 0.0289, validation loss: 0.15305\n",
      "Epoch: 4, step:   500, training loss: 0.1615, validation loss: 0.14894\n",
      "Epoch: 5, step:   600, training loss: 0.1426, validation loss: 0.14608\n",
      "Epoch: 6, step:   700, training loss: 0.0983, validation loss: 0.14390\n",
      "Epoch: 7, step:   800, training loss: 0.0564, validation loss: 0.14307\n",
      "Epoch: 8, step:   900, training loss: 0.0153, validation loss: 0.14239\n",
      "Epoch: 8, step:  1000, training loss: 0.1504, validation loss: 0.14108\n",
      "Epoch: 9, step:  1100, training loss: 0.1249, validation loss: 0.14038\n",
      "\n",
      "Estimation of severe_toxic:\n",
      "\n",
      ">> Learning: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1141, validation loss: 0.05618\n",
      "Epoch: 2, step:   200, training loss: 0.0400, validation loss: 0.05528\n",
      "Epoch: 3, step:   300, training loss: 0.0232, validation loss: 0.04641\n",
      "Epoch: 4, step:   400, training loss: 0.0072, validation loss: 0.03503\n",
      "Epoch: 4, step:   500, training loss: 0.0366, validation loss: 0.03155\n",
      "Epoch: 5, step:   600, training loss: 0.0320, validation loss: 0.02955\n",
      "Epoch: 6, step:   700, training loss: 0.0213, validation loss: 0.02835\n",
      "Epoch: 7, step:   800, training loss: 0.0116, validation loss: 0.02753\n",
      "Epoch: 8, step:   900, training loss: 0.0029, validation loss: 0.02714\n",
      "Epoch: 8, step:  1000, training loss: 0.0308, validation loss: 0.02656\n",
      "Epoch: 9, step:  1100, training loss: 0.0255, validation loss: 0.02631\n",
      "\n",
      "Estimation of obscene:\n",
      "\n",
      ">> Learning: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2449, validation loss: 0.19382\n",
      "Epoch: 2, step:   200, training loss: 0.1303, validation loss: 0.14472\n",
      "Epoch: 3, step:   300, training loss: 0.0589, validation loss: 0.11451\n",
      "Epoch: 4, step:   400, training loss: 0.0197, validation loss: 0.10166\n",
      "Epoch: 4, step:   500, training loss: 0.1079, validation loss: 0.09503\n",
      "Epoch: 5, step:   600, training loss: 0.0915, validation loss: 0.09110\n",
      "Epoch: 6, step:   700, training loss: 0.0629, validation loss: 0.08874\n",
      "Epoch: 7, step:   800, training loss: 0.0351, validation loss: 0.08708\n",
      "Epoch: 8, step:   900, training loss: 0.0097, validation loss: 0.08656\n",
      "Epoch: 8, step:  1000, training loss: 0.0924, validation loss: 0.08526\n",
      "Epoch: 9, step:  1100, training loss: 0.0766, validation loss: 0.08439\n",
      "\n",
      "Estimation of threat:\n",
      "\n",
      ">> Learning: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.0975, validation loss: 0.02008\n",
      "Epoch: 2, step:   200, training loss: 0.0166, validation loss: 0.01901\n",
      "Epoch: 3, step:   300, training loss: 0.0093, validation loss: 0.01886\n",
      "Epoch: 4, step:   400, training loss: 0.0032, validation loss: 0.01881\n",
      "Epoch: 4, step:   500, training loss: 0.0214, validation loss: 0.01861\n",
      "Epoch: 5, step:   600, training loss: 0.0182, validation loss: 0.01781\n",
      "Epoch: 6, step:   700, training loss: 0.0116, validation loss: 0.01513\n",
      "Epoch: 7, step:   800, training loss: 0.0063, validation loss: 0.01361\n",
      "Epoch: 8, step:   900, training loss: 0.0016, validation loss: 0.01258\n",
      "Epoch: 8, step:  1000, training loss: 0.0143, validation loss: 0.01114\n",
      "Epoch: 9, step:  1100, training loss: 0.0115, validation loss: 0.01078\n",
      "\n",
      "Estimation of insult:\n",
      "\n",
      ">> Learning: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.2264, validation loss: 0.17065\n",
      "Epoch: 2, step:   200, training loss: 0.1020, validation loss: 0.11670\n",
      "Epoch: 3, step:   300, training loss: 0.0502, validation loss: 0.09920\n",
      "Epoch: 4, step:   400, training loss: 0.0187, validation loss: 0.09242\n",
      "Epoch: 4, step:   500, training loss: 0.1026, validation loss: 0.08885\n",
      "Epoch: 5, step:   600, training loss: 0.0906, validation loss: 0.08875\n",
      "Epoch: 6, step:   700, training loss: 0.0618, validation loss: 0.08558\n",
      "Epoch: 7, step:   800, training loss: 0.0350, validation loss: 0.08556\n",
      "Epoch: 8, step:   900, training loss: 0.0092, validation loss: 0.08427\n",
      "Epoch: 8, step:  1000, training loss: 0.0939, validation loss: 0.08393\n",
      "Epoch: 9, step:  1100, training loss: 0.0780, validation loss: 0.08285\n",
      "\n",
      "Estimation of identity_hate:\n",
      "\n",
      ">> Learning: 112691 parameters\n",
      "\n",
      "Epoch: 1, step:   100, training loss: 0.1041, validation loss: 0.05644\n",
      "Epoch: 2, step:   200, training loss: 0.0355, validation loss: 0.05673\n",
      "Epoch: 3, step:   300, training loss: 0.0229, validation loss: 0.05430\n",
      "Epoch: 4, step:   400, training loss: 0.0088, validation loss: 0.04677\n",
      "Epoch: 4, step:   500, training loss: 0.0407, validation loss: 0.04268\n",
      "Epoch: 5, step:   600, training loss: 0.0346, validation loss: 0.03910\n",
      "Epoch: 6, step:   700, training loss: 0.0232, validation loss: 0.03687\n",
      "Epoch: 7, step:   800, training loss: 0.0131, validation loss: 0.03484\n",
      "Epoch: 8, step:   900, training loss: 0.0036, validation loss: 0.03347\n",
      "Epoch: 8, step:  1000, training loss: 0.0330, validation loss: 0.03352\n",
      "Epoch: 9, step:  1100, training loss: 0.0264, validation loss: 0.03287\n",
      "\n",
      "\n",
      "CPU times: user 1min 56s, sys: 43.7 s, total: 2min 39s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = pd.DataFrame(index=range(len(test_comments)))\n",
    "\n",
    "for target in list_classes:\n",
    "    \n",
    "    print(\"\\nEstimation of {}:\".format(target))\n",
    "\n",
    "    labels_train = train_labels[:,list_classes.index(target)]\n",
    "    labels_train = labels_train.reshape(labels_train.shape[0],1)\n",
    "\n",
    "    labels_valid = valid_labels[:,list_classes.index(target)]\n",
    "    labels_valid = labels_valid.reshape(labels_valid.shape[0],1)\n",
    "\n",
    "    labels_test = test_labels[:,list_classes.index(target)]\n",
    "    labels_test = labels_test.reshape(test_labels.shape[0],1)\n",
    "\n",
    "    use_GPU = True\n",
    "\n",
    "    batch_size = 1024\n",
    "    num_epoch = 9\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(train_comments), \n",
    "                                                   torch.FloatTensor(labels_train))\n",
    "\n",
    "    valid_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(valid_comments), \n",
    "                                                   torch.FloatTensor(labels_valid))\n",
    "\n",
    "    test_dataset = torch.FloatTensor(test_comments)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, \n",
    "                                               num_workers = 8)\n",
    "\n",
    "    net = CNN()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=0.00001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0.9)\n",
    "\n",
    "    train(num_epoch, net, train_loader, optimizer, criterion, valid_loader=valid_loader, use_GPU=use_GPU)\n",
    "\n",
    "    predictions[target] = predict(net, test_loader, use_GPU=use_GPU)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.067031562057988256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0\n",
    "\n",
    "for i in range(len(list_classes)):\n",
    "    score += log_loss(test_labels[:,i],predictions.iloc[:,i])*(1/len(list_classes))\n",
    "    \n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions.to_csv('/home/hugoperrin/Bureau/Datasets/ToxicComment/1st_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
